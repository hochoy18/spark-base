# [Spark Configuration](http://spark.apache.org/docs/latest/configuration.html)


spark 能配置的三处

 - 通过设置 [spark Properties](#1) 的 SparkConf 对象或 Java 系统属性
 - conf/spark-env.sh 等 [环境变量](#2)
 - [Logging](#3) can be configured through log4j.properties.

##  <span id = "1"> Spark Properties </span>
Spark Properties 能控制大部分的Application 设置，同时也为每个Application独立设置 配置。这些配置可以直接通过SparkConf设置（支持一些常用属性配置，e.g:master URL and  application name），也能通过 SparkConf#set() 方法设置。例如

```
val conf = new SparkConf()
             .setMaster("local[2]")
             .setAppName("CountingSheep")
val sc = new SparkContext(conf)
```
>### <span id="1.1"> Dynamically Loading Spark Properties </span>
某些场景中，可能要避免在 SparkConf 中对某些配置进行硬编码，比如说，你可能想使用不同 主机或不同的内存运行用一个Application，spark允许您简单地创建一个空的SparkConf。
```
val sc = new SparkContext(new SparkConf())
```

>### Viewing Spark Properties

>### Available Properties

>>#### Application Properties

>>#### Runtime Environment

>>#### Shuffle Behavior
>>#### Spark UI
>>#### Compression and Serialization
>>#### Memory Management
>>#### Execution Behavior
>>#### Networking
>>#### Scheduling
>>#### Dynamic Allocation

>>#### Security
>>#### Spark SQL
>>#### Spark Streaming
>>#### SparkR
>>#### GraphX
>>#### Deploy
>>#### Cluster Managers
>>>##### YARN
>>>##### Mesos
>>>##### Kubernetes
>>>##### Standalone Mode





## <span id = "2"> Environment Variables</span>

## <span id = "3"> Configuring Logging</span>

## Overriding configuration directory


## Inheriting Hadoop Cluster Configuration

## Custom Hadoop/Hive Configuration