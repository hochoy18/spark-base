### 3.4 RDD 的计算


#### 3.4.1 Task 简介


 - 原始的RDD经过一系列转换后，会在最后一个RDD上触发一个动作，这个动作会生成一个Job。在Job被划分为一批计算任务（Task）后，这批Task会被提交到集群上的计算节点去计算。
   计算节点执行计算逻辑的部分称为Executor。Executor在准备好Task的运行时环境后，会通过调用org.apache.spark.scheduler.Task#run来执行计算。Spark的Task分为两种：  
   - org.apache.spark.scheduler.ShuffleMapTask
   - org.apache.spark.scheduler.ResultTask  
    
 - 简单来说，DAG的最后一个阶段会为每个结果的Partition生成一个**ResultTask**，其余所有的阶段都会生成 **ShuffleMapTask**。生成的Task会被发送到已经启动的Executor上，由Executor来完成计算任务的执行，
 执行过程的实现在org.apache. spark.executor.Executor.TaskRunner#run。
 
 
 
#### 3.4.2 Task的执行起点
