[Spark性能优化指南——基础篇](https://tech.meituan.com/2016/04/29/spark-tuning-basic.html)


#### 避免创建重复的RDD
```
    // 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。
    
    // 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。
    // 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。
    // 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。
    val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")
    rdd1.map(...)
    val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")
    rdd2.reduce(...)
    
    // 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。
    // 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。
    // 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。
    // 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。
    val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")
    rdd1.map(...)
    rdd1.reduce(...)

```

#### 尽可能复用同一个RDD
除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，
这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。
*对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。*
``` 
    // 错误的做法。
    
    // 有一个<Long, String>格式的RDD，即rdd1。
    // 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。
    JavaPairRDD<Long, String> rdd1 = ...
    JavaRDD<String> rdd2 = rdd1.map(...)
    
    // 分别对rdd1和rdd2执行了不同的算子操作。
    rdd1.reduceByKey(...)
    rdd2.map(...)
    
    // 正确的做法。
    
    // 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。
    // 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。
    
    // 其实在这种情况下完全可以复用同一个RDD。
    // 我们可以使用rdd1，既做reduceByKey操作，也做map操作。
    // 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。
    JavaPairRDD<Long, String> rdd1 = ...
    rdd1.reduceByKey(...)
    rdd1.map(tuple._2...)
    
    // 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。
    // 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。
    // 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。
```

#### 对多次使用的RDD进行持久化  

Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。
因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。
以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。

```
// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。

// 正确的做法。
// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。
// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。
// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。
val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()
rdd1.map(...)
rdd1.reduce(...)

// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。
// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。
// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。
// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。
val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)
rdd1.map(...)
rdd1.reduce(...)

```
在RDD使用完之后可以调用```RDD.unpersist()``` 进行清缓存



#### 尽量避免使用shuffle类算子
如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，
进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。  
shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，
导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。
因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。
这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。  

Broadcast与map进行join代码示例
```
    // 传统的join操作会导致shuffle操作。
    // 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。
    val rdd3 = rdd1.join(rdd2)
    
    // Broadcast+map的join操作，不会导致shuffle操作。
    // 使用Broadcast将一个数据量较小的RDD作为广播变量。
    val rdd2Data = rdd2.collect()
    val rdd2DataBroadcast = sc.broadcast(rdd2Data)
    
    // 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。
    // 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。
    // 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。
    val rdd3 = rdd1.map(rdd2DataBroadcast...)
    
    // 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。
    // 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。
```

#### 使用map-side预聚合的shuffle操作


<div align=center><img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/5ebe0848.png" width=65% ></div><br>
<div align=center><img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/a6c7d4c4.png" width=65% ></div>  

#### 使用高性能的算子  
除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。

   - 使用reduceByKey/aggregateByKey替代groupByKey
   - 使用mapPartitions替代普通map
   - 使用foreachPartitions替代foreach
   
   - 使用filter之后进行coalesce操作
   
   - 使用 repartitionAndSortWithinPartitions 替代 repartition 与 sort 类操作
   
   
- 广播大变量


- 使用Kryo优化序列化性能


- 优化数据结构